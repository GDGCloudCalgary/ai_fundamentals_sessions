{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist =tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOYElEQVR4nO3dbYxc5XnG8euKbUwxJvHGseMQFxzjFAg0Jl0ZkBFQoVCCIgGKCLGiiFBapwlOQutKUFoVWtHKrRIiSimSKS6m4iWQgPAHmsSyECRqcFmoAROHN+MS4+0aswIDIfZ6fffDjqsFdp5dZs68eO//T1rNzLnnzLk1cPmcmeeceRwRAjD5faDTDQBoD8IOJEHYgSQIO5AEYQeSmNrOjR3i6XGoZrRzk0Aqv9Fb2ht7PFatqbDbPkfS9ZKmSPrXiFhVev6hmqGTfVYzmwRQsDE21K01fBhve4qkGyV9TtLxkpbZPr7R1wPQWs18Zl8i6fmI2BoReyXdJem8atoCULVmwn6kpF+Nery9tuwdbC+33We7b0h7mtgcgGY0E/axvgR4z7m3EbE6InojoneapjexOQDNaCbs2yXNH/X445J2NNcOgFZpJuyPSlpke4HtQyR9SdK6atoCULWGh94iYp/tFZJ+rJGhtzUR8XRlnQGoVFPj7BHxgKQHKuoFQAtxuiyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJNDWLK7qfp5b/E0/5yOyWbv+ZPz+6bm34sP3FdY9auLNYP+wbLtb/97pD6tYe7/1+cd1dw28V6yffs7JYP+bPHinWO6GpsNveJukNScOS9kVEbxVNAaheFXv234+IXRW8DoAW4jM7kESzYQ9JP7H9mO3lYz3B9nLbfbb7hrSnyc0BaFSzh/FLI2KH7TmS1tv+ZUQ8PPoJEbFa0mpJOsI90eT2ADSoqT17ROyo3e6UdJ+kJVU0BaB6DYfd9gzbMw/cl3S2pM1VNQagWs0cxs+VdJ/tA69zR0T8qJKuJpkpxy0q1mP6tGJ9xxkfKtbfPqX+mHDPB8vjxT/9dHm8uZP+49czi/V/+OdzivWNJ95Rt/bi0NvFdVcNfLZY/9hPD75PpA2HPSK2Svp0hb0AaCGG3oAkCDuQBGEHkiDsQBKEHUiCS1wrMHzmZ4r16269sVj/5LT6l2JOZkMxXKz/9Q1fLdanvlUe/jr1nhV1azNf3ldcd/qu8tDcYX0bi/VuxJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0C05/ZUaw/9pv5xfonpw1U2U6lVvafUqxvfbP8U9S3LvxB3drr+8vj5HP/6T+L9VY6+C5gHR97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHtG1E8wj1xss9q2/a6xeAlpxbru88p/9zzlCcPL9af+MYN77unA67d9bvF+qNnlMfRh197vViPU+v/APG2bxVX1YJlT5SfgPfYGBu0OwbHnMuaPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4exeYMvvDxfrwq4PF+ot31B8rf/r0NcV1l/z9N4v1OTd27ppyvH9NjbPbXmN7p+3No5b12F5v+7na7awqGwZQvYkcxt8q6d2z3l8paUNELJK0ofYYQBcbN+wR8bCkdx9Hnidpbe3+WknnV9wXgIo1+gXd3Ijol6Ta7Zx6T7S93Haf7b4h7WlwcwCa1fJv4yNidUT0RkTvNE1v9eYA1NFo2Adsz5Ok2u3O6loC0AqNhn2dpItr9y+WdH817QBolXF/N972nZLOlDTb9nZJV0taJelu25dKeknSha1scrIb3vVqU+sP7W58fvdPffkXxforN00pv8D+8hzr6B7jhj0iltUpcXYMcBDhdFkgCcIOJEHYgSQIO5AEYQeSYMrmSeC4K56tW7vkxPKgyb8dtaFYP+PCy4r1md9/pFhH92DPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+CZSmTX7168cV131p3dvF+pXX3las/8UXLyjW478/WLc2/+9+XlxXbfyZ8wzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEkzZnNzgH55arN9+9XeK9QVTD21425+6bUWxvujm/mJ939ZtDW97smpqymYAkwNhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKYuniYv2IVduL9Ts/8eOGt33sg39UrP/O39S/jl+Shp/b2vC2D1ZNjbPbXmN7p+3No5ZdY/tl25tqf+dW2TCA6k3kMP5WSeeMsfx7EbG49vdAtW0BqNq4YY+IhyUNtqEXAC3UzBd0K2w/WTvMn1XvSbaX2+6z3TekPU1sDkAzGg37TZIWSlosqV/Sd+s9MSJWR0RvRPRO0/QGNwegWQ2FPSIGImI4IvZLulnSkmrbAlC1hsJue96ohxdI2lzvuQC6w7jj7LbvlHSmpNmSBiRdXXu8WFJI2ibpaxFRvvhYjLNPRlPmzinWd1x0TN3axiuuL677gXH2RV9+8exi/fXTXi3WJ6PSOPu4k0RExLIxFt/SdFcA2orTZYEkCDuQBGEHkiDsQBKEHUiCS1zRMXdvL0/ZfJgPKdZ/HXuL9c9/8/L6r33fxuK6Byt+ShoAYQeyIOxAEoQdSIKwA0kQdiAJwg4kMe5Vb8ht/2nln5J+4cLylM0nLN5WtzbeOPp4bhg8qVg/7P6+pl5/smHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+ybn3hGL92W+Vx7pvXrq2WD/90PI15c3YE0PF+iODC8ovsH/cXzdPhT07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOPtBYOqCo4r1Fy75WN3aNRfdVVz3C4fvaqinKlw10FusP3T9KcX6rLXl353HO427Z7c93/aDtrfYftr2t2vLe2yvt/1c7XZW69sF0KiJHMbvk7QyIo6TdIqky2wfL+lKSRsiYpGkDbXHALrUuGGPiP6IeLx2/w1JWyQdKek8SQfOpVwr6fxWNQmgee/rCzrbR0s6SdJGSXMjol8a+QdB0pw66yy33We7b0h7musWQMMmHHbbh0v6oaTLI2L3RNeLiNUR0RsRvdM0vZEeAVRgQmG3PU0jQb89Iu6tLR6wPa9WnydpZ2taBFCFcYfebFvSLZK2RMR1o0rrJF0saVXt9v6WdDgJTD36t4v1139vXrF+0d/+qFj/kw/dW6y30sr+8vDYz/+l/vBaz63/VVx31n6G1qo0kXH2pZK+Iukp25tqy67SSMjvtn2ppJckXdiaFgFUYdywR8TPJI05ubuks6ptB0CrcLoskARhB5Ig7EAShB1IgrADSXCJ6wRNnffRurXBNTOK6359wUPF+rKZAw31VIUVL59WrD9+U3nK5tk/2Fys97zBWHm3YM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkGWff+wflny3e+6eDxfpVxzxQt3b2b73VUE9VGRh+u27t9HUri+se+1e/LNZ7XiuPk+8vVtFN2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJpxtm3nV/+d+3ZE+9p2bZvfG1hsX79Q2cX6x6u9+O+I4699sW6tUUDG4vrDhermEzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEo6I8hPs+ZJuk/RRjVy+vDoirrd9jaQ/lvRK7alXRUT9i74lHeGeONlM/Aq0ysbYoN0xOOaJGRM5qWafpJUR8bjtmZIes72+VvteRHynqkYBtM5E5mfvl9Rfu/+G7S2Sjmx1YwCq9b4+s9s+WtJJkg6cg7nC9pO219ieVWed5bb7bPcNaU9TzQJo3ITDbvtwST+UdHlE7JZ0k6SFkhZrZM//3bHWi4jVEdEbEb3TNL2ClgE0YkJhtz1NI0G/PSLulaSIGIiI4YjYL+lmSUta1yaAZo0bdtuWdIukLRFx3ajl80Y97QJJ5ek8AXTURL6NXyrpK5Kesr2ptuwqSctsL5YUkrZJ+lpLOgRQiYl8G/8zSWON2xXH1AF0F86gA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJDHuT0lXujH7FUn/M2rRbEm72tbA+9OtvXVrXxK9NarK3o6KiI+MVWhr2N+zcbsvIno71kBBt/bWrX1J9NaodvXGYTyQBGEHkuh02Fd3ePsl3dpbt/Yl0Vuj2tJbRz+zA2ifTu/ZAbQJYQeS6EjYbZ9j+xnbz9u+shM91GN7m+2nbG+y3dfhXtbY3ml786hlPbbX236udjvmHHsd6u0a2y/X3rtNts/tUG/zbT9oe4vtp21/u7a8o+9doa+2vG9t/8xue4qkZyV9VtJ2SY9KWhYRv2hrI3XY3iapNyI6fgKG7dMlvSnptog4obbsHyUNRsSq2j+UsyLiii7p7RpJb3Z6Gu/abEXzRk8zLul8SV9VB9+7Ql9fVBvet07s2ZdIej4itkbEXkl3STqvA310vYh4WNLguxafJ2lt7f5ajfzP0nZ1eusKEdEfEY/X7r8h6cA04x197wp9tUUnwn6kpF+Nerxd3TXfe0j6ie3HbC/vdDNjmBsR/dLI/zyS5nS4n3cbdxrvdnrXNONd8941Mv15szoR9rGmkuqm8b+lEfEZSZ+TdFntcBUTM6FpvNtljGnGu0Kj0583qxNh3y5p/qjHH5e0owN9jCkidtRud0q6T903FfXAgRl0a7c7O9zP/+umabzHmmZcXfDedXL6806E/VFJi2wvsH2IpC9JWteBPt7D9ozaFyeyPUPS2eq+qajXSbq4dv9iSfd3sJd36JZpvOtNM64Ov3cdn/48Itr+J+lcjXwj/4Kkv+xED3X6+oSkJ2p/T3e6N0l3auSwbkgjR0SXSvqwpA2Snqvd9nRRb/8u6SlJT2okWPM61NtpGvlo+KSkTbW/czv93hX6asv7xumyQBKcQQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfs4RxaLJFjqkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n",
      "(60000,)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "for data, labels in mnist.load_data():\n",
    "    print(data.shape)\n",
    "    print(data.dtype)\n",
    "    print(labels.shape)\n",
    "    print(labels.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00393124, 0.02332955, 0.02620568,\n",
       "        0.02625207, 0.17420356, 0.17566281, 0.28629534, 0.05664824,\n",
       "        0.51877786, 0.71632322, 0.77892406, 0.89301644, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05780486, 0.06524513,\n",
       "        0.16128198, 0.22713296, 0.22277047, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.368094  , 0.3747499 ,\n",
       "        0.79066747, 0.67980478, 0.61494005, 0.45002403, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.12250613, 0.45858525, 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.32420121, 0.15214552, 0.17865984,\n",
       "        0.25626376, 0.1573102 , 0.12298801, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04500225, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.32790981, 0.28826244,\n",
       "        0.26543758, 0.34149427, 0.31128482, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1541463 , 0.28272888,\n",
       "        0.18358693, 0.37314701, 0.33153488, 0.26569767, 0.01601458,\n",
       "        0.        , 0.05945042, 0.19891229, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0253731 ,\n",
       "        0.00171577, 0.22713296, 0.33153488, 0.11664776, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20500962, 0.33153488, 0.24625638, 0.00291174,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01622378, 0.24897876, 0.32790981, 0.10191096,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.04586451, 0.31235677, 0.32757096,\n",
       "        0.23335172, 0.14931733, 0.00129164, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10498298, 0.34940902,\n",
       "        0.3689874 , 0.34978968, 0.15370495, 0.04089933, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06551419,\n",
       "        0.27127137, 0.34978968, 0.32678448, 0.245396  , 0.05882702,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02333517, 0.12857881, 0.32549285, 0.41390126, 0.40743158,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32161793, 0.41390126, 0.54251585,\n",
       "        0.20001074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06697006,\n",
       "        0.18959827, 0.25300993, 0.32678448, 0.41390126, 0.45100715,\n",
       "        0.00625034, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05110617, 0.19182076, 0.33339444,\n",
       "        0.3689874 , 0.34978968, 0.32678448, 0.40899334, 0.39653769,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04117838, 0.16813739, 0.28960162, 0.32790981, 0.36833534,\n",
       "        0.3689874 , 0.34978968, 0.25961929, 0.12760592, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04431706, 0.11961607,\n",
       "        0.36545809, 0.37314701, 0.33153488, 0.32790981, 0.36833534,\n",
       "        0.28877275, 0.111988  , 0.00258328, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05298497, 0.42752138, 0.4219755 , 0.45852825,\n",
       "        0.43408872, 0.37314701, 0.33153488, 0.25273681, 0.11646967,\n",
       "        0.01312603, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.37491383,\n",
       "        0.56222061, 0.66525569, 0.63253163, 0.48748768, 0.45852825,\n",
       "        0.43408872, 0.359873  , 0.17428513, 0.01425695, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.92705966,\n",
       "        0.82698729, 0.74473314, 0.63253163, 0.4084877 , 0.24466922,\n",
       "        0.22648107, 0.02359823, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer flatten_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3684 - accuracy: 0.8997 - val_loss: 0.2200 - val_accuracy: 0.9454\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2307 - accuracy: 0.9421 - val_loss: 0.2434 - val_accuracy: 0.9404\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2007 - accuracy: 0.9504 - val_loss: 0.2915 - val_accuracy: 0.9055\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1952 - accuracy: 0.9552 - val_loss: 0.2697 - val_accuracy: 0.9452\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1824 - accuracy: 0.9581 - val_loss: 0.2430 - val_accuracy: 0.9483\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1602 - accuracy: 0.9624 - val_loss: 0.2073 - val_accuracy: 0.9525\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1541 - accuracy: 0.9643 - val_loss: 0.2403 - val_accuracy: 0.9523\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1568 - accuracy: 0.9662 - val_loss: 0.2014 - val_accuracy: 0.9612\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1604 - accuracy: 0.9649 - val_loss: 0.2255 - val_accuracy: 0.9566\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1445 - accuracy: 0.9690 - val_loss: 0.3013 - val_accuracy: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x134493e5f08>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "#model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#optim= tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#model.compile(optimizer=optim, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10,validation_split=0.2, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9511\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layer flatten_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3724 - accuracy: 0.8998 - val_loss: 0.2287 - val_accuracy: 0.9417\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2279 - accuracy: 0.9418 - val_loss: 0.2065 - val_accuracy: 0.9473\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2043 - accuracy: 0.9505 - val_loss: 0.1929 - val_accuracy: 0.9554\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1866 - accuracy: 0.9558 - val_loss: 0.2654 - val_accuracy: 0.9410\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1888 - accuracy: 0.9562 - val_loss: 0.2338 - val_accuracy: 0.9526\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1665 - accuracy: 0.9620 - val_loss: 0.2161 - val_accuracy: 0.9548\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1689 - accuracy: 0.9621 - val_loss: 0.2024 - val_accuracy: 0.9597\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9637 - val_loss: 0.2563 - val_accuracy: 0.9548\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1664 - accuracy: 0.9648 - val_loss: 0.2263 - val_accuracy: 0.9527\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 0.1587 - accuracy: 0.9646 - val_loss: 0.2305 - val_accuracy: 0.9552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13447b0ec48>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "#model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax))\n",
    "optim= tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optim, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10,validation_split=0.2, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('name.model')\n",
    "#tf.keras.models.load_model('name.model')\n",
    "predictions = model.predict(x_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slightly different approach to batching\n",
    "#representing input\n",
    "#representing output\n",
    "#different vector representations\n",
    "#different methods of representing a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28,28,1)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand_dims ~ unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.3742 - accuracy: 0.8844 - val_loss: 0.0845 - val_accuracy: 0.9765\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.1165 - accuracy: 0.9636 - val_loss: 0.0606 - val_accuracy: 0.9825\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.0874 - accuracy: 0.9740 - val_loss: 0.0511 - val_accuracy: 0.9862\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0740 - accuracy: 0.9767 - val_loss: 0.0405 - val_accuracy: 0.9887\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0636 - accuracy: 0.9796 - val_loss: 0.0394 - val_accuracy: 0.9895\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.0566 - accuracy: 0.9817 - val_loss: 0.0373 - val_accuracy: 0.9895\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0523 - accuracy: 0.9832 - val_loss: 0.0352 - val_accuracy: 0.9903\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0470 - accuracy: 0.9854 - val_loss: 0.0359 - val_accuracy: 0.9902\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.0342 - val_accuracy: 0.9898\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 19s 44ms/step - loss: 0.0423 - accuracy: 0.9861 - val_loss: 0.0332 - val_accuracy: 0.9910\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.0302 - val_accuracy: 0.9917\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0375 - accuracy: 0.9879 - val_loss: 0.0310 - val_accuracy: 0.9910\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 0.0327 - val_accuracy: 0.9912\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0312 - val_accuracy: 0.9918\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 16s 39ms/step - loss: 0.0337 - accuracy: 0.9886 - val_loss: 0.0311 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x134478b9f88>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.026390383020043373\n",
      "Test accuracy: 0.9909999966621399\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU CAN ALSO USE THE MODEL CLASS CONSTRUCT CONVENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden3Layer_MLP_Flattened_Image_Net(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Hidden3Layer_MLP_Flattened_Image_Net, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        out = self.dense2(x)\n",
    "        return out\n",
    "    \n",
    "model = Hidden3Layer_MLP_Flattened_Image_Net()\n",
    "\n",
    "optim= tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
