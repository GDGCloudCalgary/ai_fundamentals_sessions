{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ankonzoid/LearningX/blob/master/classical_RL/multiarmed_bandit/multiarmed_bandit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/solving-the-multi-armed-bandit-problem-b72de40db97c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " multiarmed_bandit.py  (author: Anson Wong / git: ankonzoid)\n",
    " We solve the multi-armed bandit problem using a classical epsilon-greedy\n",
    " agent with reward-average sampling as the estimate to action-value Q.\n",
    " This algorithm follows closely with the notation of Sutton's RL textbook.\n",
    " We set up bandit arms with fixed probability distribution of success,\n",
    " and receive stochastic rewards from each arm of +1 for success,\n",
    " and 0 reward for failure.\n",
    " The incremental update rule action-value Q for each (action a, reward r):\n",
    "   n += 1\n",
    "   Q(a) <- Q(a) + 1/n * (r - Q(a))\n",
    " where:\n",
    "   n = number of times action \"a\" was performed\n",
    "   Q(a) = value estimate of action \"a\"\n",
    "   r(a) = reward of sampling action bandit (bandit) \"a\"\n",
    " Derivation of the Q incremental update rule:\n",
    "   Q_{n+1}(a)\n",
    "   = 1/n * (r_1(a) + r_2(a) + ... + r_n(a))\n",
    "   = 1/n * ((n-1) * Q_n(a) + r_n(a))\n",
    "   = 1/n * (n * Q_n(a) + r_n(a) - Q_n(a))\n",
    "   = Q_n(a) + 1/n * (r_n(a) - Q_n(a))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-armed bandits with nActions = 10, eps = 0.1\n",
      "[Experiment 100/10000] n_steps = 500, reward_avg = 0.716\n",
      "[Experiment 200/10000] n_steps = 500, reward_avg = 0.772\n",
      "[Experiment 300/10000] n_steps = 500, reward_avg = 0.718\n",
      "[Experiment 400/10000] n_steps = 500, reward_avg = 0.734\n",
      "[Experiment 500/10000] n_steps = 500, reward_avg = 0.648\n",
      "[Experiment 600/10000] n_steps = 500, reward_avg = 0.674\n",
      "[Experiment 700/10000] n_steps = 500, reward_avg = 0.668\n",
      "[Experiment 800/10000] n_steps = 500, reward_avg = 0.668\n",
      "[Experiment 900/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 1000/10000] n_steps = 500, reward_avg = 0.704\n",
      "[Experiment 1100/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 1200/10000] n_steps = 500, reward_avg = 0.632\n",
      "[Experiment 1300/10000] n_steps = 500, reward_avg = 0.75\n",
      "[Experiment 1400/10000] n_steps = 500, reward_avg = 0.662\n",
      "[Experiment 1500/10000] n_steps = 500, reward_avg = 0.752\n",
      "[Experiment 1600/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 1700/10000] n_steps = 500, reward_avg = 0.672\n",
      "[Experiment 1800/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 1900/10000] n_steps = 500, reward_avg = 0.768\n",
      "[Experiment 2000/10000] n_steps = 500, reward_avg = 0.618\n",
      "[Experiment 2100/10000] n_steps = 500, reward_avg = 0.766\n",
      "[Experiment 2200/10000] n_steps = 500, reward_avg = 0.722\n",
      "[Experiment 2300/10000] n_steps = 500, reward_avg = 0.692\n",
      "[Experiment 2400/10000] n_steps = 500, reward_avg = 0.706\n",
      "[Experiment 2500/10000] n_steps = 500, reward_avg = 0.64\n",
      "[Experiment 2600/10000] n_steps = 500, reward_avg = 0.658\n",
      "[Experiment 2700/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 2800/10000] n_steps = 500, reward_avg = 0.742\n",
      "[Experiment 2900/10000] n_steps = 500, reward_avg = 0.714\n",
      "[Experiment 3000/10000] n_steps = 500, reward_avg = 0.77\n",
      "[Experiment 3100/10000] n_steps = 500, reward_avg = 0.726\n",
      "[Experiment 3200/10000] n_steps = 500, reward_avg = 0.71\n",
      "[Experiment 3300/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 3400/10000] n_steps = 500, reward_avg = 0.748\n",
      "[Experiment 3500/10000] n_steps = 500, reward_avg = 0.736\n",
      "[Experiment 3600/10000] n_steps = 500, reward_avg = 0.634\n",
      "[Experiment 3700/10000] n_steps = 500, reward_avg = 0.646\n",
      "[Experiment 3800/10000] n_steps = 500, reward_avg = 0.728\n",
      "[Experiment 3900/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 4000/10000] n_steps = 500, reward_avg = 0.69\n",
      "[Experiment 4100/10000] n_steps = 500, reward_avg = 0.756\n",
      "[Experiment 4200/10000] n_steps = 500, reward_avg = 0.758\n",
      "[Experiment 4300/10000] n_steps = 500, reward_avg = 0.716\n",
      "[Experiment 4400/10000] n_steps = 500, reward_avg = 0.796\n",
      "[Experiment 4500/10000] n_steps = 500, reward_avg = 0.708\n",
      "[Experiment 4600/10000] n_steps = 500, reward_avg = 0.692\n",
      "[Experiment 4700/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 4800/10000] n_steps = 500, reward_avg = 0.748\n",
      "[Experiment 4900/10000] n_steps = 500, reward_avg = 0.702\n",
      "[Experiment 5000/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 5100/10000] n_steps = 500, reward_avg = 0.758\n",
      "[Experiment 5200/10000] n_steps = 500, reward_avg = 0.694\n",
      "[Experiment 5300/10000] n_steps = 500, reward_avg = 0.742\n",
      "[Experiment 5400/10000] n_steps = 500, reward_avg = 0.732\n",
      "[Experiment 5500/10000] n_steps = 500, reward_avg = 0.716\n",
      "[Experiment 5600/10000] n_steps = 500, reward_avg = 0.704\n",
      "[Experiment 5700/10000] n_steps = 500, reward_avg = 0.762\n",
      "[Experiment 5800/10000] n_steps = 500, reward_avg = 0.7\n",
      "[Experiment 5900/10000] n_steps = 500, reward_avg = 0.698\n",
      "[Experiment 6000/10000] n_steps = 500, reward_avg = 0.73\n",
      "[Experiment 6100/10000] n_steps = 500, reward_avg = 0.768\n",
      "[Experiment 6200/10000] n_steps = 500, reward_avg = 0.714\n",
      "[Experiment 6300/10000] n_steps = 500, reward_avg = 0.742\n",
      "[Experiment 6400/10000] n_steps = 500, reward_avg = 0.768\n",
      "[Experiment 6500/10000] n_steps = 500, reward_avg = 0.706\n",
      "[Experiment 6600/10000] n_steps = 500, reward_avg = 0.57\n",
      "[Experiment 6700/10000] n_steps = 500, reward_avg = 0.742\n",
      "[Experiment 6800/10000] n_steps = 500, reward_avg = 0.754\n",
      "[Experiment 6900/10000] n_steps = 500, reward_avg = 0.78\n",
      "[Experiment 7000/10000] n_steps = 500, reward_avg = 0.702\n",
      "[Experiment 7100/10000] n_steps = 500, reward_avg = 0.712\n",
      "[Experiment 7200/10000] n_steps = 500, reward_avg = 0.678\n",
      "[Experiment 7300/10000] n_steps = 500, reward_avg = 0.758\n",
      "[Experiment 7400/10000] n_steps = 500, reward_avg = 0.668\n",
      "[Experiment 7500/10000] n_steps = 500, reward_avg = 0.622\n",
      "[Experiment 7600/10000] n_steps = 500, reward_avg = 0.714\n",
      "[Experiment 7700/10000] n_steps = 500, reward_avg = 0.664\n",
      "[Experiment 7800/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 7900/10000] n_steps = 500, reward_avg = 0.72\n",
      "[Experiment 8000/10000] n_steps = 500, reward_avg = 0.78\n",
      "[Experiment 8100/10000] n_steps = 500, reward_avg = 0.73\n",
      "[Experiment 8200/10000] n_steps = 500, reward_avg = 0.796\n",
      "[Experiment 8300/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 8400/10000] n_steps = 500, reward_avg = 0.614\n",
      "[Experiment 8500/10000] n_steps = 500, reward_avg = 0.738\n",
      "[Experiment 8600/10000] n_steps = 500, reward_avg = 0.728\n",
      "[Experiment 8700/10000] n_steps = 500, reward_avg = 0.674\n",
      "[Experiment 8800/10000] n_steps = 500, reward_avg = 0.698\n",
      "[Experiment 8900/10000] n_steps = 500, reward_avg = 0.766\n",
      "[Experiment 9000/10000] n_steps = 500, reward_avg = 0.71\n",
      "[Experiment 9100/10000] n_steps = 500, reward_avg = 0.77\n",
      "[Experiment 9200/10000] n_steps = 500, reward_avg = 0.66\n",
      "[Experiment 9300/10000] n_steps = 500, reward_avg = 0.76\n",
      "[Experiment 9400/10000] n_steps = 500, reward_avg = 0.744\n",
      "[Experiment 9500/10000] n_steps = 500, reward_avg = 0.784\n",
      "[Experiment 9600/10000] n_steps = 500, reward_avg = 0.702\n",
      "[Experiment 9700/10000] n_steps = 500, reward_avg = 0.676\n",
      "[Experiment 9800/10000] n_steps = 500, reward_avg = 0.768\n",
      "[Experiment 9900/10000] n_steps = 500, reward_avg = 0.71\n",
      "[Experiment 10000/10000] n_steps = 500, reward_avg = 0.744\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs  # success probabilities for each arm\n",
    "\n",
    "    def step(self, action):\n",
    "        # Pull arm and get stochastic reward (1 for success, 0 for failure)\n",
    "        return 1 if (np.random.random()  < self.probs[action]) else 0\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nActions, eps):\n",
    "        self.nActions = nActions\n",
    "        self.eps = eps\n",
    "        self.n = np.zeros(nActions, dtype=np.int) # action counts n(a)\n",
    "        self.Q = np.zeros(nActions, dtype=np.float) # value Q(a)\n",
    "\n",
    "    def update_Q(self, action, reward):\n",
    "        # Update Q action-value given (action, reward)\n",
    "        self.n[action] += 1\n",
    "        self.Q[action] += (1.0/self.n[action]) * (reward - self.Q[action])\n",
    "\n",
    "    def get_action(self):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.random() < self.eps: # explore\n",
    "            return np.random.randint(self.nActions)\n",
    "        else: # exploit\n",
    "            return np.random.choice(np.flatnonzero(self.Q == self.Q.max()))\n",
    "\n",
    "# Start multi-armed bandit simulation\n",
    "def experiment(probs, N_episodes):\n",
    "    env = Environment(probs) # initialize arm probabilities\n",
    "    agent = Agent(len(env.probs), eps)  # initialize agent\n",
    "    actions, rewards = [], []\n",
    "    for episode in range(N_episodes):\n",
    "        action = agent.get_action() # sample policy\n",
    "        reward = env.step(action) # take step + get reward\n",
    "        agent.update_Q(action, reward) # update Q\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "    return np.array(actions), np.array(rewards)\n",
    "\n",
    "# Settings\n",
    "probs = [0.10, 0.50, 0.60, 0.80, 0.10,\n",
    "         0.25, 0.60, 0.45, 0.75, 0.65] # bandit arm probabilities of success\n",
    "N_experiments = 10000 # number of experiments to perform\n",
    "N_steps = 500 # number of steps (episodes)\n",
    "eps = 0.1 # probability of random exploration (fraction)\n",
    "save_fig = True # save file in same directory\n",
    "output_dir = os.path.join(os.getcwd(), \"output\")\n",
    "\n",
    "# Run multi-armed bandit experiments\n",
    "print(\"Running multi-armed bandits with nActions = {}, eps = {}\".format(len(probs), eps))\n",
    "R = np.zeros((N_steps,))  # reward history sum\n",
    "A = np.zeros((N_steps, len(probs)))  # action history sum\n",
    "for i in range(N_experiments):\n",
    "    actions, rewards = experiment(probs, N_steps)  # perform experiment\n",
    "    if (i + 1) % (N_experiments / 100) == 0:\n",
    "        print(\"[Experiment {}/{}] \".format(i + 1, N_experiments) +\n",
    "              \"n_steps = {}, \".format(N_steps) +\n",
    "              \"reward_avg = {}\".format(np.sum(rewards) / len(rewards)))\n",
    "    R += rewards\n",
    "    for j, a in enumerate(actions):\n",
    "        A[j][a] += 1\n",
    "\n",
    "# Plot reward results\n",
    "R_avg =  R / np.float(N_experiments)\n",
    "plt.plot(R_avg, \".\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "plt.xlim([1, N_steps])\n",
    "if save_fig:\n",
    "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, \"rewards.png\"), bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot action results\n",
    "for i in range(len(probs)):\n",
    "    A_pct = 100 * A[:,i] / N_experiments\n",
    "    steps = list(np.array(range(len(A_pct)))+1)\n",
    "    plt.plot(steps, A_pct, \"-\",\n",
    "             linewidth=4,\n",
    "             label=\"Arm {} ({:.0f}%)\".format(i+1, 100*probs[i]))\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Count Percentage (%)\")\n",
    "leg = plt.legend(loc='upper left', shadow=True)\n",
    "plt.xlim([1, N_steps])\n",
    "plt.ylim([0, 100])\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(4.0)\n",
    "if save_fig:\n",
    "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, \"actions.png\"), bbox_inches=\"tight\")\n",
    "else:\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
