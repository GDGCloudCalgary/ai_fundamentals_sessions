{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits:https://gluon-nlp.mxnet.io/examples/language_model/language_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Language Models\n",
    "\n",
    "A statistical language model is simply a probability distribution over sequences of words or characters [1].\n",
    "In this tutorial, we'll restrict our attention to word-based language models.\n",
    "Given a reliable language model, we can answer questions like *which among the following strings are we more likely to encounter?*\n",
    "\n",
    "1. 'On Monday, Mr. Lamar’s “DAMN.” took home an even more elusive honor,\n",
    "one that may never have even seemed within reach: the Pulitzer Prize\"\n",
    "1. \"Frog zealot flagged xylophone the bean wallaby anaphylaxis extraneous\n",
    "porpoise into deleterious carrot banana apricot.\"\n",
    "\n",
    "Even if we've never seen either of these sentences in our entire lives, and even though no rapper has previously been\n",
    "awarded a Pulitzer Prize, we wouldn't be shocked to see the first sentence in the New York Times.\n",
    "By comparison, we can all agree that the second sentence, consisting of incoherent babble, is comparatively unlikely.\n",
    "A statistical language model can assign precise probabilities to each of these and other strings of words.\n",
    "\n",
    "Given a large corpus of text, we can estimate (or, in this case, train) a language model $\\hat{p}(x_1, ..., x_n)$.\n",
    "And given such a model, we can sample strings $\\mathbf{x} \\sim \\hat{p}(x_1, ..., x_n)$, generating new strings according to their estimated probability.\n",
    "Among other useful applications, we can use language models to score candidate transcriptions from speech recognition models, given a preference to sentences that seem more probable (at the expense of those deemed anomalous).\n",
    " In this notebook, we will go through an example of using GluonNLP to\n",
    "\n",
    "(i) implement a typical LSTM language model architecture\n",
    "(ii) train the language model on a corpus of real data\n",
    "(iii) bring in your own dataset for training\n",
    "(iv) grab off-the-shelf pre-trained state-of-the-art language models (i.e., AWD language model) using GluonNLP.\n",
    "\n",
    "## What is a language model (LM)?\n",
    "\n",
    "The standard approach to language modeling consists of training a model that given a trailing window of text, predicts the next word in the sequence.\n",
    "When we train the model we feed in the inputs $x_1, x_2, ...$ and try at each time step to predict the corresponding next word $x_2, ..., x_{n+1}$.\n",
    "To generate text from a language model, we can iteratively predict the next word, and then feed this word as an input to the model at the subsequent time step. The image included below demonstrates this idea.\n",
    "\n",
    "<img src=\"https://gluon.mxnet.io/_images/recurrent-lm.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Train your own language model\n",
    "\n",
    "Now let's go through the step-by-step process on how to train your own\n",
    "language model using GluonNLP.\n",
    "\n",
    "\n",
    "### Preparation\n",
    "\n",
    "We'll start by taking care of\n",
    "our basic dependencies and setting up our environment.\n",
    "\n",
    "Firstly, we import the required modules for GluonNLP and the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp\n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the environment for GluonNLP.\n",
    "\n",
    "Please note that we should change num_gpus according to how many NVIDIA GPUs are available on the target machine in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cpu(0)]\n"
     ]
    }
   ],
   "source": [
    "num_gpus = 1\n",
    "#context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "context = [mx.cpu()]\n",
    "print(context)\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup the hyperparameters for the LM we are using.\n",
    "\n",
    "Note that BPTT stands for \"back propagation through time,\" and LR stands for learning rate. A link to more information on truncated BPTT can be found [here.](https://en.wikipedia.org/wiki/Backpropagation_through_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25                     #gradient clipping to control exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Now, we load the dataset, extract the vocabulary, numericalize, and batchify in order to perform truncated BPTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "# Extract the vocabulary and numericalize with \"Counter\"\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "# Batchify for BPTT\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we load the pre-defined language model architecture as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)\n",
    "\n",
    "# Initialize the model\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "# Initialize the trainer and optimizer and specify some hyperparameters\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "\n",
    "# Specify the loss function, in this case, cross-entropy with softmax.\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the LM\n",
    "\n",
    "Now that everything is ready, we can start training the model.\n",
    "\n",
    "We first define a helper function for detaching the gradients on specific states for easier truncated BPTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then a helper evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that ctx is short for context\n",
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main training loop\n",
    "\n",
    "Our loss function will be the standard cross-entropy loss function used for multi-class classification, applied at each time step to compare the model's predictions to the true next word in the sequence.\n",
    "We can calculate gradients with respect to our parameters using truncated BPTT.\n",
    "In this case, we'll back propagate for $35$ time steps, updating our weights with stochastic gradient descent and a learning rate of $20$; these correspond to the hyperparameters that we specified earlier in the notebook.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for actually training the model\n",
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context,\n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context,\n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now actually perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 7.65, ppl 2110.68, throughput 19.53 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 6.76, ppl 862.18, throughput 18.42 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 6.36, ppl 576.31, throughput 18.12 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 6.19, ppl 486.11, throughput 18.75 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 6.05, ppl 424.46, throughput 18.64 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.97, ppl 390.07, throughput 18.84 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.86, ppl 350.31, throughput 18.81 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.86, ppl 350.21, throughput 18.67 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.70, ppl 299.63, throughput 18.75 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.68, ppl 291.77, throughput 18.50 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.57, ppl 263.69, throughput 19.18 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.58, ppl 264.81, throughput 19.27 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.57, ppl 263.43, throughput 18.39 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.46, ppl 235.61, throughput 18.20 samples/s\n",
      "[Epoch 0] throughput 18.74 samples/s\n",
      "[Epoch 0] time cost 3278.68s, valid loss 5.43, valid ppl 228.04\n",
      "test loss 5.35, test ppl 210.50\n",
      "[Epoch 1 Batch 200/2983] loss 5.47, ppl 238.26, throughput 18.90 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.46, ppl 233.93, throughput 19.12 samples/s\n",
      "[Epoch 1 Batch 600/2983] loss 5.29, ppl 197.45, throughput 19.24 samples/s\n",
      "[Epoch 1 Batch 800/2983] loss 5.31, ppl 201.70, throughput 18.79 samples/s\n",
      "[Epoch 1 Batch 1000/2983] loss 5.28, ppl 196.32, throughput 19.45 samples/s\n",
      "[Epoch 1 Batch 1200/2983] loss 5.26, ppl 193.06, throughput 19.33 samples/s\n",
      "[Epoch 1 Batch 1400/2983] loss 5.27, ppl 193.98, throughput 19.35 samples/s\n",
      "[Epoch 1 Batch 1600/2983] loss 5.33, ppl 206.28, throughput 19.16 samples/s\n",
      "[Epoch 1 Batch 1800/2983] loss 5.20, ppl 181.39, throughput 19.12 samples/s\n",
      "[Epoch 1 Batch 2000/2983] loss 5.22, ppl 184.18, throughput 19.20 samples/s\n",
      "[Epoch 1 Batch 2200/2983] loss 5.12, ppl 167.68, throughput 19.23 samples/s\n",
      "[Epoch 1 Batch 2400/2983] loss 5.15, ppl 172.98, throughput 19.32 samples/s\n",
      "[Epoch 1 Batch 2600/2983] loss 5.17, ppl 176.35, throughput 19.42 samples/s\n",
      "[Epoch 1 Batch 2800/2983] loss 5.09, ppl 162.85, throughput 19.17 samples/s\n",
      "[Epoch 1] throughput 19.21 samples/s\n",
      "[Epoch 1] time cost 3197.83s, valid loss 5.18, valid ppl 176.96\n",
      "test loss 5.10, test ppl 164.03\n",
      "[Epoch 2 Batch 200/2983] loss 5.14, ppl 171.10, throughput 19.27 samples/s\n",
      "[Epoch 2 Batch 400/2983] loss 5.16, ppl 174.55, throughput 19.43 samples/s\n",
      "[Epoch 2 Batch 600/2983] loss 4.99, ppl 147.35, throughput 19.23 samples/s\n",
      "[Epoch 2 Batch 800/2983] loss 5.04, ppl 154.35, throughput 19.43 samples/s\n",
      "[Epoch 2 Batch 1000/2983] loss 5.03, ppl 152.58, throughput 19.30 samples/s\n",
      "[Epoch 2 Batch 1200/2983] loss 5.03, ppl 152.38, throughput 19.30 samples/s\n",
      "[Epoch 2 Batch 1400/2983] loss 5.06, ppl 156.86, throughput 19.00 samples/s\n",
      "[Epoch 2 Batch 1600/2983] loss 5.11, ppl 166.46, throughput 19.21 samples/s\n",
      "[Epoch 2 Batch 1800/2983] loss 4.99, ppl 147.06, throughput 19.41 samples/s\n",
      "[Epoch 2 Batch 2000/2983] loss 5.02, ppl 151.64, throughput 19.39 samples/s\n",
      "[Epoch 2 Batch 2200/2983] loss 4.93, ppl 138.26, throughput 19.37 samples/s\n",
      "[Epoch 2 Batch 2400/2983] loss 4.97, ppl 143.43, throughput 19.43 samples/s\n",
      "[Epoch 2 Batch 2600/2983] loss 4.99, ppl 146.78, throughput 19.26 samples/s\n",
      "[Epoch 2 Batch 2800/2983] loss 4.92, ppl 136.48, throughput 19.12 samples/s\n",
      "[Epoch 2] throughput 19.27 samples/s\n",
      "[Epoch 2] time cost 3190.73s, valid loss 5.06, valid ppl 157.33\n",
      "test loss 4.98, test ppl 146.10\n",
      "Total training throughput 17.92 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own dataset\n",
    "\n",
    "When we train a language model, we fit to the statistics of a given dataset.\n",
    "While many papers focus on a few standard datasets, such as WikiText or the Penn Tree Bank, that's just to provide a standard benchmark for the purpose of comparing models against one another.\n",
    "In general, for any given use case, you'll want to train your own language model using a dataset of your own choice.\n",
    "Here, for demonstration, we'll grab some `.txt` files corresponding to Sherlock Holmes novels.\n",
    "\n",
    "We first download the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./sherlockholmes.train.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt...\n",
      "Downloading ./sherlockholmes.valid.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt...\n",
      "Downloading ./sherlockholmes.test.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt...\n",
      "Downloading ./tinyshakespeare/input.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt...\n",
      "['sherlockholmes.valid.txt', 'sherlockholmes.train.txt', 'sherlockholmes.test.txt']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"./sherlockholmes.train.txt\"\n",
    "VALID_PATH = \"./sherlockholmes.valid.txt\"\n",
    "TEST_PATH = \"./sherlockholmes.test.txt\"\n",
    "PREDICT_PATH = \"./tinyshakespeare/input.txt\"\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt\",\n",
    "    TRAIN_PATH,\n",
    "    sha1_hash=\"d65a52baaf32df613d4942e0254c81cff37da5e8\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt\",\n",
    "    VALID_PATH,\n",
    "    sha1_hash=\"71133db736a0ff6d5f024bb64b4a0672b31fc6b3\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt\",\n",
    "    TEST_PATH,\n",
    "    sha1_hash=\"b7ccc4778fd3296c515a3c21ed79e9c2ee249f70\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\",\n",
    "    PREDICT_PATH,\n",
    "    sha1_hash=\"04486597058d11dcc2c556b1d0433891eb639d2e\")\n",
    "\n",
    "print(glob.glob(\"sherlockholmes.*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify the tokenizer as well as batchify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "moses_tokenizer = nlp.data.SacreMosesTokenizer()\n",
    "\n",
    "sherlockholmes_datasets = [\n",
    "    nlp.data.CorpusDataset(\n",
    "        'sherlockholmes.{}.txt'.format(name),\n",
    "        sample_splitter=nltk.tokenize.sent_tokenize,\n",
    "        tokenizer=moses_tokenizer,\n",
    "        flatten=True,\n",
    "        eos='<eos>') for name in ['train', 'valid', 'test']\n",
    "]\n",
    "\n",
    "sherlockholmes_train_data, sherlockholmes_val_data, sherlockholmes_test_data = [\n",
    "    bptt_batchify(dataset) for dataset in sherlockholmes_datasets\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the evaluation to see whether our previous model trained on the other dataset does well on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.77, test ppl 117.60\n"
     ]
    }
   ],
   "source": [
    "sherlockholmes_L = evaluate(model, sherlockholmes_val_data, batch_size,\n",
    "                            context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f' %\n",
    "      (sherlockholmes_L, math.exp(sherlockholmes_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we have the option of training the model on the new dataset with just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 892.88 samples/s\n",
      "[Epoch 0] time cost 3.86s, valid loss 3.00, valid ppl 20.08\n",
      "test loss 2.93, test ppl 18.80\n",
      "[Epoch 1] throughput 890.10 samples/s\n",
      "[Epoch 1] time cost 3.88s, valid loss 3.13, valid ppl 22.92\n",
      "Learning rate now 5.000000\n",
      "[Epoch 2] throughput 882.91 samples/s\n",
      "[Epoch 2] time cost 3.91s, valid loss 2.74, valid ppl 15.41\n",
      "test loss 2.70, test ppl 14.89\n",
      "Total training throughput 736.79 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    sherlockholmes_train_data, # This is your input training data, we leave batchifying and tokenizing as an exercise for the reader\n",
    "    sherlockholmes_val_data,\n",
    "    sherlockholmes_test_data, # This would be your test data, again left as an exercise for the reader\n",
    "    epochs=3,\n",
    "    lr=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pre-trained AWD LSTM language model\n",
    "\n",
    "AWD LSTM language model is the state-of-the-art RNN language model [1]. The main technique leveraged is to add weight-dropout on the recurrent hidden to hidden matrices to prevent overfitting on the recurrent connections.\n",
    "\n",
    "#### Load the vocabulary and the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /root/.mxnet/models/3963101239443680508/3963101239443680508_wikitext-2-be36dc52.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wikitext-2-be36dc52.zip...\n",
      "Downloading /root/.mxnet/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip2338dea9-cb82-4ae9-a9dc-cbc743bf1c9c from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip...\n",
      "AWDRNN(\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(400 -> 33278, linear)\n",
      "  )\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 400, float32)\n",
      "    (1): Dropout(p = 0.65, axes=(0,))\n",
      "  )\n",
      "  (encoder): HybridSequential(\n",
      "    (0): LSTM(400 -> 1150, TNC)\n",
      "    (1): LSTM(1150 -> 1150, TNC)\n",
      "    (2): LSTM(1150 -> 400, TNC)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "awd_model_name = 'awd_lstm_lm_1150'\n",
    "awd_model, vocab = nlp.model.get_model(\n",
    "    awd_model_name,\n",
    "    vocab=vocab,\n",
    "    dataset_name=dataset_name,\n",
    "    pretrained=True,\n",
    "    ctx=context[0])\n",
    "\n",
    "print(awd_model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the pre-trained model on the validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.23, val ppl 68.80\n",
      "Best test loss 4.19, test ppl 65.73\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate(awd_model, val_data, batch_size, context[0])\n",
    "test_L = evaluate(awd_model, test_data, batch_size, context[0])\n",
    "\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a cache LSTM LM\n",
    "\n",
    "Cache LSTM language model [2] adds a cache-like memory to neural network language models. It can be used in conjunction with the aforementioned AWD LSTM language model or other LSTM models.\n",
    "It exploits the hidden outputs to define a probability distribution over the words in the cache.\n",
    "It generates  state-of-the-art results at inference time.\n",
    "\n",
    "<img src=cache_model.png width=\"500\">\n",
    "\n",
    "#### Load the pre-trained model and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheCell(\n",
      "  (lm_model): AWDRNN(\n",
      "    (decoder): HybridSequential(\n",
      "      (0): Dense(400 -> 33278, linear)\n",
      "    )\n",
      "    (embedding): HybridSequential(\n",
      "      (0): Embedding(33278 -> 400, float32)\n",
      "      (1): Dropout(p = 0.65, axes=(0,))\n",
      "    )\n",
      "    (encoder): HybridSequential(\n",
      "      (0): LSTM(400 -> 1150, TNC)\n",
      "      (1): LSTM(1150 -> 1150, TNC)\n",
      "      (2): LSTM(1150 -> 400, TNC)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "window = 2\n",
    "theta = 0.662\n",
    "lambdas = 0.1279\n",
    "bptt = 2000\n",
    "cache_model = nlp.model.train.get_cache_model(name=awd_model_name,\n",
    "                                             dataset_name=dataset_name,\n",
    "                                             window=window,\n",
    "                                             theta=theta,\n",
    "                                             lambdas=lambdas,\n",
    "                                             ctx=context[0])\n",
    "\n",
    "print(cache_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define specific get_batch and evaluation helper functions for the cache model\n",
    "\n",
    "Note that these helper functions are very similar to the ones we defined above, but are slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_batch_size = 1\n",
    "val_test_batchify = nlp.data.batchify.CorpusBatchify(vocab, val_test_batch_size)\n",
    "val_data = val_test_batchify(val_dataset)\n",
    "test_data = val_test_batchify(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data_source, i, seq_len=None):\n",
    "    seq_len = min(seq_len if seq_len else bptt, len(data_source) - 1 - i)\n",
    "    data = data_source[i:i + seq_len]\n",
    "    target = data_source[i + 1:i + 1 + seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cache(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    next_word_history = None\n",
    "    cache_history = None\n",
    "    for i in range(0, len(data_source) - 1, bptt):\n",
    "        if i > 0:\n",
    "            print('Batch %d, ppl %f' % (i, math.exp(total_L / i)))\n",
    "        if i == bptt:\n",
    "            return total_L / i\n",
    "        data, target = get_batch(data_source, i)\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        L = 0\n",
    "        outs, next_word_history, cache_history, hidden = model(\n",
    "            data, target, next_word_history, cache_history, hidden)\n",
    "        for out in outs:\n",
    "            L += (-mx.nd.log(out)).asscalar()\n",
    "        total_L += L / data.shape[1]\n",
    "        hidden = detach(hidden)\n",
    "    return total_L / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the pre-trained model on the validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 60.767825\n",
      "Batch 2000, ppl 67.390510\n",
      "Best validation loss 4.11, val ppl 60.77\n",
      "Best test loss 4.21, test ppl 67.39\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "\n",
    "print('Best validation loss %.2f, val ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Merity, S., et al. “Regularizing and optimizing LSTM language models”. ICLR 2018\n",
    "\n",
    "[2] Grave, E., et al. “Improving neural language models with a continuous cache”. ICLR 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
