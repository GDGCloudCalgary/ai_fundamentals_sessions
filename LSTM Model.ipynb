{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import  torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1037e-05, 6.3016e-10, 6.6770e+22],\n",
      "        [2.1006e+20, 5.1432e-11, 4.2330e+21]])\n",
      "Parameter containing:\n",
      "tensor([[1.1037e-05, 6.3016e-10, 6.6770e+22],\n",
      "        [2.1006e+20, 5.1432e-11, 4.2330e+21]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor(2,3))\n",
    "print(nn.Parameter(torch.Tensor(2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter :  when they're assigned as Module attributes they are automatically added to the list of its parameters, \n",
    "                and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
    "                States are saved using this subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz:int, hidden_sz:int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        #i_t\n",
    "        self.u_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.v_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #f_t\n",
    "        self.u_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.v_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_t\n",
    "        self.u_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.v_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #o_t\n",
    "        self.u_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.v_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "    #default implementation in nn.module\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        batch_size, seq_size, feature_length = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
    "                        torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "    \n",
    "    \n",
    "        for t in range(seq_size):\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(x_t * self.U_i + h_t * self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t * self.U_f + h_t * self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t * self.U_c + h_t * self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t * self.U_o + h_t * self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        #reshape hidden_seq p/ retornar\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001FD4C4753C8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_obj = CustomLSTM(2,2)\n",
    "lstm_obj.parameters(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape => (batch_size, sequence_length, feature_length)\n",
    "\n",
    "if sequence is a sentence : \"I have a cat\"\n",
    "sequence length is 4. In practice we use a standard sequence length say 20, \n",
    "and pad the sequences or prune the sequences\n",
    "each word is a feature - and has a feature length\n",
    "\n",
    "e.g. embedding vector of length 300\n",
    "    one-hot encoding where everty word is a 0 ot 1,\n",
    "    in that case the feature length of a word is the size of the full vocabulary\n",
    "    i.e. the total numbe of words\n",
    "    \n",
    "batch: how many sequences are there in a batch\n",
    "    \n",
    "    \n",
    "    \n",
    "The weight matrix multiplies each element of the sequence:\n",
    "shape(feature_length, length_of_hidden_state)\n",
    "The hidden_state for each element in the sequence has shape (batch_size, length_of_hidden_state)\n",
    "Output shape (batch_size, sequence length,length_of_hidden_state)\n",
    "the weight matrix that will multiply output must have shape: (length_of_hidden_state, length_of_hidden_state)\n",
    "\n",
    "    \n",
    "The feedforward operation receives ht, ct parameters.\n",
    "Set to zero if nothing is carried forward.\n",
    "Feed forward LSTM equations for each of the sequence elements (each word or feature vector in the sequence) preserving the ht, ct \n",
    "Introducing it as the states for the next element of the sequence.\n",
    "\n",
    "Terminate:return the predictions and the last states tuple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "         \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            gates = x_t * self.W + h_t * self.U + self.bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.tanh(gates[:, HS*2:HS*3]),\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, peephole=False):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.peephole = peephole\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, \n",
    "                init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "         \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            \n",
    "            if self.peephole:\n",
    "                gates = x_t * U + c_t * V + bias\n",
    "            else:\n",
    "                gates = x_t * U + h_t * V + bias\n",
    "                g_t = torch.tanh(gates[:, HS*2:HS*3])\n",
    "            \n",
    "            i_t, f_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            \n",
    "            if self.peephole:\n",
    "                c_t = f_t * c_t + i_t * torch.sigmoid(x_t @ U + bias)[:, HS*2:HS*3]\n",
    "                h_t = torch.tanh(o_t * c_t)\n",
    "            else:\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "                \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
